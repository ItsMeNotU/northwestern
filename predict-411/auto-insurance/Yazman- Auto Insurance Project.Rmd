---
title: "Unit 2: Logistic Regression and Auto Insurance Claims"
author: 'Josh Yazman'
output: html_notebook
---
# Introduction
The primary purpose of this assignment is to predict the likelihood of an auto insurance customer being involved in an accident. First, missing data is accounted for and, where possible, null values and outliers are imputed using variable-specific regression tree models. Then three logistic regression and probit models are fit and evaluated. Additionally, a boosted classification tree model is developed to attempt to improve predictive accuracy. Finally, `R` code is provided to score a fresh data file and make predictions for teams not included in the original sample. All code used in the development of this project can be found [here](https://github.com/joshyazman/northwestern/tree/master/predict-411/auto-insurance). 

## Exploratory Data Analysis and Prep
### Missing Data
The dataset includes information on 8,161 insurance customers. The variables included are listed below as well as their theoretical effects.

```{r, echo = FALSE, message=FALSE, warning = FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
setwd('C:/Users/joshy/Google Drive/northwestern/PREDICT-411/general-docs/moneyball/')
library(tidyverse)
library(yaztheme)
library(reshape2)
library(ggridges)
# library(randomForest)
# library(moments)
library(readxl)
train <- read_csv('logit_insurance.csv')%>%
  mutate(INCOME = as.numeric(gsub('[$,]','',INCOME)),
         HOME_VAL = as.numeric(gsub('[$,]','',HOME_VAL)),
         BLUEBOOK = as.numeric(gsub('[$,]','',BLUEBOOK)),
         OLDCLAIM = as.numeric(gsub('[$,]','',OLDCLAIM)),
         PARENT1 = tolower(gsub('[z_<]','', PARENT1)),
         MSTATUS = tolower(gsub('[z_<]','', MSTATUS)),
         SEX = tolower(gsub('[z_<]','', SEX)),
         EDUCATION = tolower(gsub('[z_<]','', EDUCATION)),
         JOB = tolower(gsub('[z_<]','', JOB)),
         CAR_USE = tolower(gsub('[z_<]','', CAR_USE)),
         CAR_TYPE = tolower(gsub('[z_<]','', CAR_TYPE)),
         RED_CAR = tolower(gsub('[z_<]','', RED_CAR)),
         REVOKED = tolower(gsub('[z_<]','', REVOKED)),
         URBANICITY = tolower(gsub('[z_<]','', URBANICITY)))
colnames(train) <- tolower(colnames(train))
dict <- read_xlsx('DataDictionary_Insurance.xlsx')
knitr::kable(dict, format = 'pandoc')
# test <- read_csv('logit_insurance_test.csv')
```

Some basic explanatory plots and correlations are presented below. 26% of customers have been involved in an accident. Many of the variables follow a nice, normal distribution like age and tragel time. But others, like home value, and income are positively skewed. The data will need to be appropriately transformed to be useful for predictive purposes. No single numeric variable has an incredibly high correlation with claim likelihood, but number of past claims and moving violation points are closest - explaining about 20% of the variation in crashes. 
```{r, warning = FALSE, fig.width=12}
GGally::ggpairs(train_for_sum%>%dplyr::select(-index, -target_amt)%>%select_if(is.numeric))
```

The data is largely complete. The variables with the most null values are only 5-6% incomplete. 
```{r}
## Your first step is to turn outliers into null values 
nulls <- data.frame(col = as.character(colnames(train)), 
                    pct_null = colSums(is.na(train))*100/(colSums(is.na(train))+colSums(!is.na(train))))%>%
  filter(col != 'INDEX')
ggplot(nulls, aes(x = col, y = pct_null))+
  geom_bar(fill = yaz_cols[1], stat = 'identity')+
  coord_flip()+
  labs(title = 'Figure 1: Distribution of Missing Data',
       x = element_blank(), y = 'Percent of Information Missing')+
  theme_yaz()+
  ylim(0,100)
```

Regression and classification trees are used to impute missing values. Additionally, outlier variables are deleted and imputed using the same methods. Binary flags are generated for values imputed for each possible reason. Finally, all values are scaled and centered to stabilize model estimates.
```{r}
colnames(train) <- tolower(colnames(train))

train.flagged <- train%>%
  mutate_all(funs(na.flag = ifelse(is.na(.),1,0)))
int_df <- train.flagged%>%
  dplyr::select(-index, -target_flag, -target_amt)%>%
  dplyr::select_if(is.numeric)
# md.pattern(int_df)
cleaned_cols <- list()
for(c in colnames(train%>%
                  dplyr::select(-index, -target_flag, -target_amt, -kidsdriv)%>%
                  dplyr::select_if(is.numeric))){
  column <- train.flagged%>%select_(col = c)
  iqr <- quantile(column$col, na.rm = T)[4] - quantile(column$col, na.rm = T)[2]
  low <- quantile(column$col, na.rm = T)[2] - iqr
  high <- quantile(column$col, na.rm = T)[4] + iqr
  
  vals <- c()
  for(i in seq(1:nrow(int_df))){
    ifelse(between(column$col[i], low - (1.5*iqr), high + (1.5*iqr)),
           vals[i] <- column$col[i], 
           ifelse(is.na(column$col[i]), vals[i] <- NA, vals[i] <- NA))
  }
  
  ifelse(length(vals) == nrow(int_df),
         cleaned_cols[[c]] <- vals, 
         cleaned_cols[[c]] <- c(vals,NA))
}

df2 <- bind_cols(
  bind_cols(cleaned_cols)%>%
    # select(-kidsdriv)%>%
    scale(center = TRUE)%>%
    data.frame(),
  train.flagged%>%
    dplyr::select(ends_with('na.flag'), kidsdriv),
  train%>%dplyr::select_if(is.character)
  )

df3 <- df2%>%
  mutate(
    kidsdriv_out.flag = ifelse(is.na(kidsdriv) & kidsdriv_na.flag ==0,1,0),
    age_out.flag = ifelse(is.na(age) & age_na.flag ==0,1,0),
    homekids_out.flag = ifelse(is.na(homekids) & homekids_na.flag ==0,1,0),
    yoj_out.flag = ifelse(is.na(yoj) & yoj_na.flag ==0,1,0),
    income_out.flag = ifelse(is.na(income) & income_na.flag ==0,1,0),
    parent1_out.flag = ifelse(is.na(parent1) & parent1_na.flag ==0,1,0),
    home_val_out.flag = ifelse(is.na(home_val) & home_val_na.flag ==0,1,0),
    mstatus_out.flag = ifelse(is.na(mstatus) & mstatus_na.flag ==0,1,0),
    sex_out.flag = ifelse(is.na(sex) & sex_na.flag ==0,1,0),
    education_out.flag = ifelse(is.na(education) & education_na.flag ==0,1,0),
    job_out.flag = ifelse(is.na(job) & job_na.flag ==0,1,0),
    travtime_out.flag = ifelse(is.na(travtime) & travtime_na.flag ==0,1,0),
    car_use_out.flag = ifelse(is.na(car_use) & car_use_na.flag ==0,1,0),
    bluebook_out.flag = ifelse(is.na(bluebook) & bluebook_na.flag ==0,1,0),
    tif_out.flag = ifelse(is.na(tif) & tif_na.flag ==0,1,0),
    car_type_out.flag = ifelse(is.na(car_type) & car_type_na.flag ==0,1,0),
    red_car_out.flag = ifelse(is.na(red_car) & red_car_na.flag ==0,1,0),
    oldclaim_out.flag = ifelse(is.na(oldclaim) & oldclaim_na.flag ==0,1,0),
    clm_freq_out.flag = ifelse(is.na(clm_freq) & clm_freq_na.flag ==0,1,0),
    revoked_out.flag = ifelse(is.na(revoked) & revoked_na.flag ==0,1,0),
    mvr_pts_out.flag = ifelse(is.na(mvr_pts) & mvr_pts_na.flag ==0,1,0),
    car_age_out.flag = ifelse(is.na(car_age) & car_age_na.flag ==0,1,0),
    urbanicity_out.flag = ifelse(is.na(urbanicity) & urbanicity_na.flag ==0,1,0)
)

library(mice)
temp_df <- mice(df3, method = 'cart', maxit = 1)
train_clean <- complete(temp_df)%>%
  bind_cols(train%>%dplyr::select(index, target_flag, target_amt))
```

# Model Development
Three models are developed as part of this project - a logistic regression model using heuristic variable selection, a logistic regression model with variables selected through a bi-directional stepwise process meant to minimize AIC, and a probit regression model using the same variable selection technique.

## Heuristic Logistic
The first model developed is a logistic regression model taking all inputs where the data dictionary contains some theoretical impact on accident likelihood. Logistic regression models work similarly to OLS regression models, but the OLS formula is transformed and inverted to produce probabilities of binary outcomes rather than point estimates of a continuous variable[^1].

### Performance
Figures 2.1 and 2.2 illustrate a troubling tendency to return incorrectly low accident probabilities. Too many innacurately low predictions could lead to the company taking on more claims than premiums.  Additionally, those records with high probability scores (in the 90s) don't tend to have more accidents than some lower score groupings on aggregate (in the 70s and 80s). 
```{r, echo=FALSE, fig.height = 3, fig.width= 11}
method[1] <- 'Heuristic'

preds.list <- list()
mcfaddenrsq <- c()
ks.stat <- c()
auc <- c()
rmse <- c()

for(i in 1:1000){
  train.temp <- sample_frac(train_clean, .8, replace = TRUE)
  test.temp <- train_clean%>%filter(!index %in% train.temp$index)
  
  model.log1 <- glm(target_flag ~ age + car_use + clm_freq + home_val + 
                      job + kidsdriv + mvr_pts + oldclaim + red_car + 
                      revoked + sex + tif + travtime + yoj, 
                    train.temp, family=binomial("logit"))
  
  preds <- predict(model.log1, test.temp, type="response")
  
  log1.preds <- data.frame(
    index = test.temp$index,
    actual = test.temp$target_flag,
    preds
    )%>%
    mutate(residual = preds - actual)
  preds.list[[i]] <- log1.preds
  
  mcfaddenrsq[i] <- as.numeric(pscl::pR2(model.log1)[4])
  ks.stat[i] <- ks.test(preds, "pnorm", 1, 2)$statistic
  auc[i] <- pROC::roc(target_flag ~ preds, data = test.temp)$auc[1]
  rmse[i] <- sqrt(mean(log1.preds$residual^2, na.rm = T))
}

boot.log1.preds  <- bind_rows(preds.list)

log1gof <- data.frame(mcfaddenrsq, ks.stat, auc, rmse)%>%
  mutate(method = method[1])

log.dist <- ggplot(boot.log1.preds%>%
                     group_by(index, actual)%>%
                     summarise(mean.pred = mean(preds, na.rm = T))%>%
                     mutate(residual = mean.pred - actual), 
                   aes(x = mean.pred, fill = as.character(actual)))+
    geom_density(alpha = .75)+
    labs(title = 'Fig 2.1: Log. Reg Donation Prop.',
         y = 'Frequency',
         x = 'Claim Likelihood Score',
         subtitle = 'Predicted vs. Actual')+
    theme_yaz(base_size = 10)+
    scale_fill_manual(values = yaz_cols[4:5], 
                      name = 'Validated Classification', 
                      labels = c('No Claim','Claimant'))

log.ridges <- ggplot(boot.log1.preds%>%
         group_by(rounded = as.numeric(substr(as.character(preds),1,3)))%>%
         mutate(pct_grouped = mean(actual, na.rm =T),
                group_n = n())%>%
         ungroup()%>%
         filter(!is.na(rounded)), 
       aes(x = pct_grouped, y = reorder(as.character(rounded), rounded),
           alpha = group_n))+
  geom_density_ridges(fill = yaz_cols[1], weight = .5)+
  theme_yaz()+
  labs(title = 'Fig 2.2: Log. Reg. Score Accuracy',
         y = 'Predicted Probability',
         x = 'Proportion Who Are Claimants',
         subtitle = paste0('Predicted values in the verification set',
                           '\n(Darker plots indicate greater frequencies).'))+
  theme(legend.position = 'none')+
  xlim(0,1)

# Display the distribution of residuals
log1.ecdf <- ggplot(boot.log1.preds%>%
                     group_by(index, actual)%>%
                     summarise(mean.pred = mean(preds, na.rm = T)), 
                   aes(x = mean.pred))+
  stat_ecdf(geom = 'point', color = yaz_cols[1])+
  theme_yaz()+
  labs(title = 'Fig 2.3: Heuristic ECDF',
       y = 'Accident Frequency', x = 'Accident Probability',
       subtitle = ' ')


grid.arrange(log.dist, log.ridges, log1.ecdf,nrow = 1)
```

### Model Composition
The strongest variable in the model is whether or not a customer's driver's license has been revoked in the past seven years. Kid drivers and a preponderance of moving violation points also contribute to claim likelihood which makes sense. Almost all variable coefficients are significant and perform according to the assumptions made in the data dictionary.

```{r}
model.log1 <- glm(target_flag ~ age + car_use + clm_freq + home_val + 
                      job + kidsdriv + mvr_pts + oldclaim + red_car + 
                      revoked + sex + tif + travtime + yoj, 
                    train_clean, family=binomial("logit"))

mod.log1.parts <- summary(model.log1)$coefficients%>%
  data.frame()%>%
  bind_cols(data.frame(vars = rownames(summary(model.log1)$coefficients)))%>%
  dplyr::select(est = Estimate, se = Std..Error, p = Pr...z.., vars)

ggplot(mod.log1.parts, aes(x = reorder(vars, est), y = est, color = p))+
  geom_errorbar(aes(ymin = est - se, ymax = est + se),
                width = 0, size = 2)+
  labs(y = 'Coefficient Estimate (+- 1 Standard Error)',
       title = 'Model Coefficient Estimates',
       x = element_blank())+
  theme_yaz()+
  coord_flip()+
  scale_color_continuous(high = yaz_cols[4], low = yaz_cols[3],
                         breaks = c(0, .001, .01, .05, .1),guide = 'legend')+
  geom_hline(yintercept = 0, linetype = 'dashed')
```

## Bidirectional Variable Selection Logistic Regression
The second model tested is a logistic regression model developed with a backward variable selection process. Starting with a logit model using all possible variables the model iteratively works through possible sets of predictor variables, adding and dropping variables to minimize AIC[^2][^3]. It's a computationally expensive process, which could become a factor in production decisions. However, the gains to predictive accuracy may be worthwhile. 

### Performance
Figure 3.1 looks nearly identical to 2.1 - especially the distributions of scores for claimants which skew too low. The cumulative density plot (Figure 3.3) looks slightly more skewed than that in 2.3. But the predictive accuracy illustrated in Figure 3.2 demonstrates significany improvement over the heursitc model. 
```{r, echo=FALSE, fig.height = 3, fig.width= 11}
# Tutorial: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4842399/
library(MASS)
library(dummies)

data.for.stepaic <- train_clean%>%
  dplyr::select(-index_na.flag, -target_flag_na.flag, -target_amt_na.flag, 
         -target_amt, -index)%>%
  select_if(is.numeric)%>%
  bind_cols(
    dummy.data.frame(train_clean%>%select_if(is.character))
  )

full.mod <- glm(target_flag~., data = data.for.stepaic, family = binomial)

step <- stepAIC(full.mod, trace = T, direction = 'both')
# print the formula and bootstrap it!
# paste0('glm(target_flag~', paste(names(step$coefficients)[-1], collapse = '+'),
#        ', data = train.temp), family=binomial("logit")')))

method[2] <- 'Bidirectional Stepwise (Logit)'

preds.list.2 <- list()
mcfaddenrsq <- c()
ks.stat <- c()
auc <- c()
rmse <- c()

for(i in 1:1000){
  train.temp <- sample_frac(data.for.stepaic, .8, replace = TRUE)
  test.temp <- data.for.stepaic%>%
    bind_cols(data.frame(index = train_clean$index))%>%
    filter(!index %in% train.temp$index)
  
  model.log2 <- glm(target_flag~homekids+yoj+income+home_val+travtime+bluebook+tif+oldclaim+
                      clm_freq+mvr_pts+age_na.flag+kidsdriv+oldclaim_out.flag+mvr_pts_out.flag+
                      parent1no+mstatusno+`educationhigh school`+`jobblue collar`+
                      jobclerical+jobdoctor+jobmanager+car_usecommercial+car_typeminivan+
                      `car_typesports car`+car_typesuv+revokedno+
                      `urbanicityhighly rural/ rural`, 
                    train.temp, family=binomial("logit"))
  
  preds <- predict(model.log2, test.temp, type="response")

  log2.preds <- data.frame(
    index = test.temp$index,
    actual = test.temp$target_flag,
    preds
    )%>%
    mutate(residual = preds - actual)
  preds.list.2[[i]] <- log1.preds
  
  mcfaddenrsq[i] <- as.numeric(pscl::pR2(model.log2)[4])
  ks.stat[i] <- ks.test(preds, "pnorm", 1, 2)$statistic
  auc[i] <- pROC::roc(target_flag ~ preds, data = test.temp)$auc[1]
  rmse[i] <- sqrt(mean(log2.preds$residual^2, na.rm = T))
}

boot.log2.preds  <- bind_rows(preds.list.2)

log2gof <- data.frame(mcfaddenrsq, ks.stat, auc, rmse)%>%
  mutate(method = method[2])

log2.dist <- ggplot(boot.log2.preds%>%
                     group_by(index, actual)%>%
                     summarise(mean.pred = mean(preds, na.rm = T))%>%
                     mutate(residual = mean.pred - actual), 
                   aes(x = mean.pred, fill = as.character(actual)))+
    geom_density(alpha = .75)+
    labs(title = 'Fig 3.1: Bi-Step Accident Prop.',
         y = 'Frequency',
         x = 'Claim Likelihood Score',
         subtitle = 'Predicted vs. Actual')+
    theme_yaz(base_size = 10)+
    scale_fill_manual(values = yaz_cols[4:5], 
                      name = 'Validated Classification', 
                      labels = c('No Claim','Claimant'))

log2.ridges <- ggplot(boot.log2.preds%>%
         group_by(rounded = as.numeric(substr(as.character(preds),1,3)))%>%
         mutate(pct_grouped = mean(actual, na.rm =T),
                group_n = n())%>%
         ungroup()%>%
         filter(!is.na(rounded)), 
       aes(x = pct_grouped, y = reorder(as.character(rounded), rounded),
           alpha = group_n))+
  geom_density_ridges(fill = yaz_cols[1], weight = .5)+
  theme_yaz()+
  labs(title = 'Fig 3.2: Bi-Step Score Accuracy',
         y = 'Predicted Probability',
         x = 'Proportion Who Are Claimants',
         subtitle = paste0('Predicted values in the verification set',
                           '\n(Darker plots indicate greater frequencies).'))+
  theme(legend.position = 'none')+
  xlim(0,1)

log2.ecdf <- ggplot(boot.log2.preds%>%
                     group_by(index, actual)%>%
                     summarise(mean.pred = mean(preds, na.rm = T)), 
                   aes(x = mean.pred))+
  stat_ecdf(geom = 'point', color = yaz_cols[1])+
  theme_yaz()+
  labs(title = 'Fig 3.3: Bi-Step ECDF',
       y = 'Accident Frequency', x = 'Accident Probability',
       subtitle = ' ')


grid.arrange(log2.dist, log2.ridges, log2.ecdf, nrow = 1)
```

### Model Composition
Many of the variables selected through the stepwise process perform similarly to the assumptions laid out in the data dictionary. Some of the variable coefficients (like missing age data, SUV ownership, and presence of kids in the household) are not significant, but they're left in the model to minimize AIC.

```{r}
model.log2 <- glm(target_flag~homekids+yoj+income+home_val+travtime+bluebook+tif+oldclaim+
                      clm_freq+mvr_pts+age_na.flag+kidsdriv+oldclaim_out.flag+mvr_pts_out.flag+
                      parent1no+mstatusno+`educationhigh school`+`jobblue collar`+
                      jobclerical+jobdoctor+jobmanager+car_usecommercial+car_typeminivan+
                      `car_typesports car`+car_typesuv+revokedno+
                      `urbanicityhighly rural/ rural`, 
                    data.for.stepaic, family=binomial("logit"))

model.log2.parts <- summary(model.log2)$coefficients%>%
  data.frame()%>%
  bind_cols(data.frame(vars = rownames(summary(model.log2)$coefficients)))%>%
  dplyr::select(est = Estimate, se = Std..Error, p = Pr...z.., vars)

ggplot(model.log2.parts, aes(x = reorder(vars, est), y = est, color = p))+
  geom_errorbar(aes(ymin = est - se, ymax = est + se),
                width = 0, size = 2)+
  labs(y = 'Coefficient Estimate (+- 1 Standard Error)',
       title = 'Model Coefficient Estimates',
       x = element_blank())+
  theme_yaz()+
  coord_flip()+
  scale_color_continuous(high = yaz_cols[4], low = yaz_cols[3],
                         breaks = c(0, .001, .01, .05, .1, 1),guide = 'legend')+
  geom_hline(yintercept = 0, linetype = 'dashed')
```

## Bidirectional Variable Selection Probit Regression
The Probit model uses the probit link function to modify OLS regression for classification purposes (similar to the logit process but with different transformations). In this case, the models performed similarly. A bi-directional stepwise variable selection technique is used to select a subset of variables that minimizes AIC. Then the winning model is iteratively applied to subsamples of the data and the average predicted values are calculated for each record.

### Performance
Similar to the bi-directional logistic regression model, too many non-accident predictions are clearly a problem in this data set. Figures 4.1 and 4.3 illustrate the relative success of the model at positively identifying non-accident cases but also that a significantly high number of cases where accidents did received low scores. One bright spot in this model compared to the previous models is Figure 4.2 which illustrates that as scores go up (in aggregate) accident likelihood does tend to go up as well. 

```{r probit, echo=FALSE, fig.height=3, fig.width=11, message=FALSE, warning=FALSE}
# Tutorial: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4842399/
full.mod <- glm(target_flag~., data = data.for.stepaic, family = binomial("probit"))

step <- stepAIC(full.mod, trace = T, direction = 'both')
# print the formula and bootstrap it!
# paste0('glm(target_flag~', paste(names(step$coefficients)[-1], collapse = '+'),
#        ', data = train.temp), family=binomial("logit")')))

method[3] <- 'Bidirectional Stepwise (Probit)'

preds.list.3 <- list()
mcfaddenrsq <- c()
ks.stat <- c()
auc <- c()
rmse <- c()

for(i in 1:1000){
  train.temp <- sample_frac(data.for.stepaic, .8, replace = TRUE)
  test.temp <- data.for.stepaic%>%
    bind_cols(data.frame(index = train_clean$index))%>%
    filter(!index %in% train.temp$index)
  
  model.prob <- glm(target_flag~homekids+yoj+income+home_val+travtime+bluebook+tif+oldclaim+
                      clm_freq+mvr_pts+age_na.flag+kidsdriv+oldclaim_out.flag+mvr_pts_out.flag+
                      parent1no+mstatusno+`educationhigh school`+`jobblue collar`+
                      jobclerical+jobdoctor+jobmanager+car_usecommercial+car_typeminivan+
                      `car_typesports car`+car_typesuv+revokedno+
                      `urbanicityhighly rural/ rural`, 
                    train.temp, family=binomial("probit"))
  
  preds <- predict(model.prob, test.temp, type="response")
  
  prob.preds <- data.frame(
    index = test.temp$index,
    actual = test.temp$target_flag,
    preds
    )%>%
    mutate(residual = preds - actual)
  preds.list.3[[i]] <- prob.preds
  
  mcfaddenrsq[i] <- as.numeric(pscl::pR2(model.log2)[4])
  ks.stat[i] <- ks.test(preds, "pnorm", 1, 2)$statistic
  auc[i] <- pROC::roc(target_flag ~ preds, data = test.temp)$auc[1]
  rmse[i] <- sqrt(mean(log2.preds$residual^2, na.rm = T))
}

boot.prob.preds  <- bind_rows(preds.list.3)
propgof <- data.frame(mcfaddenrsq, ks.stat, auc, rmse)%>%
  mutate(method = method[3])

prob.dist <- ggplot(boot.prob.preds%>%
                     group_by(index, actual)%>%
                     summarise(mean.pred = mean(preds, na.rm = T))%>%
                     mutate(residual = mean.pred - actual), 
                   aes(x = mean.pred, fill = as.character(actual)))+
    geom_density(alpha = .75)+
    labs(title = 'Fig 4.1: Probit Accident Prop.',
         y = 'Frequency',
         x = 'Claim Likelihood Score',
         subtitle = 'Predicted vs. Actual')+
    theme_yaz(base_size = 10)+
    scale_fill_manual(values = yaz_cols[4:5], 
                      name = 'Validated Classification', 
                      labels = c('No Claim','Claimant'))

prob.ridges <- ggplot(boot.prob.preds%>%
         group_by(rounded = as.numeric(substr(as.character(preds),1,3)))%>%
           mutate(pct_grouped = mean(actual, na.rm =T),
                  group_n = n())%>%
           ungroup()%>%
           filter(!is.na(rounded),
                  rounded < 1), 
       aes(x = pct_grouped, y = reorder(as.character(rounded), rounded),
           alpha = group_n))+
  geom_density_ridges(fill = yaz_cols[1], weight = .5)+
  theme_yaz()+
  labs(title = 'Fig 4.2: Probit Score Accuracy',
         y = 'Predicted Probability',
         x = 'Proportion Who Are Claimants',
         subtitle = paste0('Predicted values in the verification set',
                           '\n(Darker plots indicate greater frequencies).'))+
  theme(legend.position = 'none')+
  xlim(0,1)

prob.ecdf <- ggplot(boot.prob.preds%>%
                     group_by(index, actual)%>%
                     summarise(mean.pred = mean(preds, na.rm = T)), 
                   aes(x = mean.pred))+
  stat_ecdf(geom = 'point', color = yaz_cols[1])+
  theme_yaz()+
  labs(title = 'Fig 4.3: Probit ECDF',
       y = 'Accident Frequency', x = 'Accident Probability',
       subtitle = ' ')

grid.arrange(prob.dist, prob.ridges, prob.ecdf, nrow = 1)
```

### Model Composition
The probit model coefficients behave similarly to the stepwise logistic regression model. Again, regardless of statistical significance, the variables selected through the AIC minimization process are retained in the model.

```{r}
model.prob <- glm(target_flag~homekids+yoj+income+home_val+travtime+bluebook+tif+oldclaim+
                      clm_freq+mvr_pts+age_na.flag+kidsdriv+oldclaim_out.flag+mvr_pts_out.flag+
                      parent1no+mstatusno+`educationhigh school`+`jobblue collar`+
                      jobclerical+jobdoctor+jobmanager+car_usecommercial+car_typeminivan+
                      `car_typesports car`+car_typesuv+revokedno+
                      `urbanicityhighly rural/ rural`, 
                    data.for.stepaic, family=binomial("probit"))

model.prob.parts <- summary(model.prob)$coefficients%>%
  data.frame()%>%
  bind_cols(data.frame(vars = rownames(summary(model.prob)$coefficients)))%>%
  dplyr::select(est = Estimate, se = Std..Error, p = Pr...z.., vars)

ggplot(model.prob.parts, aes(x = reorder(vars, est), y = est, color = p))+
  geom_errorbar(aes(ymin = est - se, ymax = est + se),
                width = 0, size = 2)+
  labs(y = 'Coefficient Estimate (+- 1 Standard Error)',
       title = 'Model Coefficient Estimates',
       x = element_blank())+
  theme_yaz()+
  coord_flip()+
  scale_color_continuous(high = yaz_cols[4], low = yaz_cols[3],
                         breaks = c(0, .001, .01, .05, .1, 1),guide = 'legend')+
  geom_hline(yintercept = 0, linetype = 'dashed')
```

## Model Evaluation
The three models are evaluated based on the McFadden Pseudo $R^2$ statistic, the Kolmagorov - Smirnov (KS) statistic, area under the curve (AUC), and Root-Mean Squared Error (RMSE). All calculations are cross-validated using 1,000 iterations of sampled model training and out-of-sample model testing.

McFadden offers a stand-in for the kind of $R^2$ used to evaluate OLS regression models[^4]. The Probit model explains the greatest amount of variance followed closely by the bi-directional logit model. The heuristic logit model trailed significantly.

The KS statistic compares the cumulative distribution of the model predictions to the ideal distribution for that data. In this case, the heuristic model performed best of all three. While the KS-statistic is a more formal test for appropriateness of the cumulative distribution curve, Figures 2.3, 3.3, and 4.3 all indicated significant depression of scores towards lower-end probabilities than were appropriate. The heuristic model was the closest by appearance to the ideal distribution and the KS-statistic confirms that hunch. 

AUC "relates the hit rate to the false alarm rate" in classification modeling [^5]. The heuristic model was far, far behind on this model and produced highly unstable scores. For production purposes, consistent scores will ease adoption of the model, so to the extent that political concerns factor into model selection, the inconsistency inthe AUC measures for the heuristic model are problematic. 

Finally, cross-validated RMSE tests hold-out sample predictions against known values. The bidirectional logit model slightly outperforms the bidirectional probit model, but not by any significant margin. That said, the heuristic model lags significantly. 

```{r, fig.width=10, fig.height=5}
gof_comb <- bind_rows(log1gof, log2gof, propgof)

mcfaddenrsq.plot <- ggplot(gof_comb, aes(x = mcfaddenrsq, y = method))+
  geom_density_ridges(fill = yaz_cols[5], alpha = .7)+
  labs(title = expression(paste("McFadden's ", R^{2})),
       x = expression(paste("McFadden's ", R^{2})),
       y = element_blank())+
  theme_yaz()
kstest.plot <- ggplot(gof_comb, aes(x = ks.stat, y = method))+
  geom_density_ridges(fill = yaz_cols[5], alpha = .7)+
  labs(title = 'KS - Test',
       x = 'KS - Statistic',
       y = element_blank())+
  theme_yaz()
auc.plot <- ggplot(gof_comb, aes(x = auc, y = method))+
  geom_density_ridges(fill = yaz_cols[5], alpha = .7)+
  labs(title = 'Area Under (ROC)',
       x = 'AUC',
       y = element_blank())+
  theme_yaz()
rmse.plot <- ggplot(gof_comb, aes(x = rmse, y = method))+
  geom_density_ridges(fill = yaz_cols[5], alpha = .7)+
  labs(title = 'RMSE',
       x = 'RMSE',
       y = element_blank())+
  theme_yaz()

grid.arrange(mcfaddenrsq.plot, kstest.plot, auc.plot, rmse.plot,
             nrow = 2)
```

Step-wise variable selection is a computationally expensive process, but the gains in predictive accuracy more than justify the added time required to train these models. Both logistic regression and probit regression are easy enough to explain to management if the predictions are correct. Therefore the winning model is the Bidirectional Stepwise Probit method.

# Appendices
## Appendix A: Code to Process New Data
The below function (and package dependencies) take a file of new data and returns a file with the index, predicted likelihood of a claim, and a rough estimate of likely claim cost. The cost estimate is the median value of claims.
```{r}
library(tidyverse)
library(yaztheme)
library(reshape2)
library(ggridges)
library(randomForest)
library(moments)
library(mice)
library(MASS)
library(dummies)
library(dplyr)

score_creator <- function(infile){
  train <- read_csv(infile)%>%
    mutate(INCOME = as.numeric(gsub('[$,]','',INCOME)),
           HOME_VAL = as.numeric(gsub('[$,]','',HOME_VAL)),
           BLUEBOOK = as.numeric(gsub('[$,]','',BLUEBOOK)),
           OLDCLAIM = as.numeric(gsub('[$,]','',OLDCLAIM)),
           PARENT1 = tolower(gsub('[z_<]','', PARENT1)),
           MSTATUS = tolower(gsub('[z_<]','', MSTATUS)),
           SEX = tolower(gsub('[z_<]','', SEX)),
           EDUCATION = tolower(gsub('[z_<]','', EDUCATION)),
           JOB = tolower(gsub('[z_<]','', JOB)),
           CAR_USE = tolower(gsub('[z_<]','', CAR_USE)),
           CAR_TYPE = tolower(gsub('[z_<]','', CAR_TYPE)),
           RED_CAR = tolower(gsub('[z_<]','', RED_CAR)),
           REVOKED = tolower(gsub('[z_<]','', REVOKED)),
           URBANICITY = tolower(gsub('[z_<]','', URBANICITY)))

  colnames(train) <- tolower(colnames(train))
  
  train.flagged <- train%>%
    mutate_all(funs(na.flag = ifelse(is.na(.),1,0)))
  int_df <- train.flagged%>%
    dplyr::select(-index, -target_flag, -target_amt)%>%
    dplyr::select_if(is.numeric)
  # md.pattern(int_df)
  cleaned_cols <- list()
  for(c in colnames(train%>%
                    dplyr::select(-index, -target_flag, -target_amt, -kidsdriv)%>%
                    dplyr::select_if(is.numeric))){
    column <- train.flagged%>%select_(col = c)
    iqr <- quantile(column$col, na.rm = T)[4] - quantile(column$col, na.rm = T)[2]
    low <- quantile(column$col, na.rm = T)[2] - iqr
    high <- quantile(column$col, na.rm = T)[4] + iqr
    
    vals <- c()
    for(i in seq(1:nrow(int_df))){
      ifelse(between(column$col[i], low - (1.5*iqr), high + (1.5*iqr)),
             vals[i] <- column$col[i], 
             ifelse(is.na(column$col[i]), vals[i] <- NA, vals[i] <- NA))
    }
    
    ifelse(length(vals) == nrow(int_df),
           cleaned_cols[[c]] <- vals, 
           cleaned_cols[[c]] <- c(vals,NA))
  }
  
  df2 <- bind_cols(
    bind_cols(cleaned_cols)%>%
      # select(-kidsdriv)%>%
      scale(center = TRUE)%>%
      data.frame(),
    train.flagged%>%
      dplyr::select(ends_with('na.flag'), kidsdriv),
    train%>%dplyr::select_if(is.character)
    )
  
  df3 <- df2%>%
    mutate(
      kidsdriv_out.flag = ifelse(is.na(kidsdriv) & kidsdriv_na.flag ==0,1,0),
      age_out.flag = ifelse(is.na(age) & age_na.flag ==0,1,0),
      homekids_out.flag = ifelse(is.na(homekids) & homekids_na.flag ==0,1,0),
      yoj_out.flag = ifelse(is.na(yoj) & yoj_na.flag ==0,1,0),
      income_out.flag = ifelse(is.na(income) & income_na.flag ==0,1,0),
      parent1_out.flag = ifelse(is.na(parent1) & parent1_na.flag ==0,1,0),
      home_val_out.flag = ifelse(is.na(home_val) & home_val_na.flag ==0,1,0),
      mstatus_out.flag = ifelse(is.na(mstatus) & mstatus_na.flag ==0,1,0),
      sex_out.flag = ifelse(is.na(sex) & sex_na.flag ==0,1,0),
      education_out.flag = ifelse(is.na(education) & education_na.flag ==0,1,0),
      job_out.flag = ifelse(is.na(job) & job_na.flag ==0,1,0),
      travtime_out.flag = ifelse(is.na(travtime) & travtime_na.flag ==0,1,0),
      car_use_out.flag = ifelse(is.na(car_use) & car_use_na.flag ==0,1,0),
      bluebook_out.flag = ifelse(is.na(bluebook) & bluebook_na.flag ==0,1,0),
      tif_out.flag = ifelse(is.na(tif) & tif_na.flag ==0,1,0),
      car_type_out.flag = ifelse(is.na(car_type) & car_type_na.flag ==0,1,0),
      red_car_out.flag = ifelse(is.na(red_car) & red_car_na.flag ==0,1,0),
      oldclaim_out.flag = ifelse(is.na(oldclaim) & oldclaim_na.flag ==0,1,0),
      clm_freq_out.flag = ifelse(is.na(clm_freq) & clm_freq_na.flag ==0,1,0),
      revoked_out.flag = ifelse(is.na(revoked) & revoked_na.flag ==0,1,0),
      mvr_pts_out.flag = ifelse(is.na(mvr_pts) & mvr_pts_na.flag ==0,1,0),
      car_age_out.flag = ifelse(is.na(car_age) & car_age_na.flag ==0,1,0),
      urbanicity_out.flag = ifelse(is.na(urbanicity) & urbanicity_na.flag ==0,1,0)
  )
  
  temp_df <- mice(df3, method = 'cart', maxit = 1)
  train_clean <- complete(temp_df)%>%
    bind_cols(train%>%dplyr::select(index, target_flag, target_amt))

  data.for.stepaic <- train_clean%>%
    dplyr::select(-index_na.flag, -target_flag_na.flag, -target_amt_na.flag, 
           -target_amt)%>%
    dplyr::select_if(is.numeric)%>%
    bind_cols(
      dummy.data.frame(train_clean%>%select_if(is.character))
    )
  
  med.claim <- 4104.00

  prob.preds <- data.frame(
    P_TARGET_FLAG = predict(model.prob, data.for.stepaic, type="response")
    )%>%
    bind_cols(data.for.stepaic)%>%
    mutate(P_TARGET_AMT = med.claim)%>%
    dplyr::select(INDEX = index, P_TARGET_FLAG, P_TARGET_AMT)

  return(prob.preds)
}

test_data <- score_creator('logit_insurance_test.csv')
write.csv(x = test_data, file = 'yazman_insurance_test.csv')
```

# Sources
[^1]: Hoffmann, John P. Generalized Linear Models: an Applied Approach. Pearson/Allyn & Bacon, 2004.
[^2]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4842399/
[^3]: James, Gareth, et al. An Introduction to Statistical Learning with Applications in R. Springer, 2017.
[^4]: https://mathewanalytics.com/2015/09/02/logistic-regression-in-r-part-two/
[^5]: https://www.kdnuggets.com/2010/09/pub-is-auc-the-best-measure.html